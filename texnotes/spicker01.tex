\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Lineares Ausgleichsproblem}: \linebreak[3]
	Daten:
	\begin{tabular}{r|c|c|c|c}
		$t_i$ & 0 & 1 & 2 & 3 \\ 
		\hline 
		$y_i$ & 3 & 2,14 & 1,8 & 1,72 \\ 
	\end{tabular}
	\mbox{Modellfunktion}:
	$ y(t) = \alpha\nicefrac{1}{1+t} +\beta$
	\begin{dmath*}
		A = \begin{pmatrix}
			y(t_1 = 0)\\
			y(1)\\
			y(2)\\
			y(t_n = 3)\\
		\end{pmatrix}
		\cdot
		\begin{pmatrix}
			\alpha\\
			\beta
		\end{pmatrix}
		\hiderel{;}\linebreak[2]
		b \hiderel{=}
		\begin{pmatrix}
			y_1 = 3\\
			2,14\\
			1,86\\
			y_n = 1,72
		\end{pmatrix}
		\linebreak[3]
		\leadsto
		\begin{pmatrix}
			1&1\\
			\nicefrac{1}{2}&1\\
			\nicefrac{1}{3}&1\\
			\nicefrac{1}{4}&1\\
		\end{pmatrix}
		\cdot
		\begin{pmatrix}
			\alpha\\
			\beta
		\end{pmatrix}
		\hiderel{=}
		\begin{pmatrix}
			y_1 = 3\\
			2,14\\
			1,86\\
			y_n = 1,72
		\end{pmatrix}
	\end{dmath*}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Satz: 1.1}: \linebreak[3]
	$x^*\in \R$ ist genau dann eine Lösung des linearen Ausgleichsproblems, 
	wenn $x^*$ Lösung der Normalgleichung $A^TAx = A^Tb$ ist.
	Es gibt mindestens eine Lösung $x^*$.
	Sie ist eindeutig, gdw. $Rang(A) = n$.
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Satz: 1.2}: \linebreak[3]
	Sei $A\in\R^{m\times n}$, $b\in\R^{m}$ mit QR-Zerlegung von $A$, 
	$Rang(A)\equiv n$, 
	$A=QR$, $R = \begin{pmatrix} R_1\\ 0 \end{pmatrix}$, 
	\mbox{$R_1\in\R^{n\times n}$}
	und $\begin{pmatrix}c_1\\c_2\end{pmatrix} \coloneqq Q^Tb$ mit \mbox{$c_1\in\R^n$}, 
	\mbox{$c_2\in\R^{m-n}$}. \linebreak[3]
	Dann gilt: \linebreak[3]
	$R_1$ ist regulär und \linebreak[3]
%	\mbox{$x^* = R_1^{-1}\cdot c_1$} ist die eindeutige Lösung des Linearen Ausgleichsproblems $\norm{b-Ax}^2_2 = min$.
	Außerdem gilt: $\norm{b-Ax}_2 = \norm{c_2}$.
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Hessenbergmatrix durch Householder-Reflexion}: \linebreak[3]
	Mithilfe einer Householder-Reflexion, dargestellt durch Matrixmultiplikation $Q_u \cdot A$, 
	kann ein Teil der Matrix zu null transformiert werden.\linebreak[3]	
	Mit $v$ als Spaltenvektor von $A$, welcher die erste Spalte enthält, 
	wird \mbox{$u \coloneqq v + \text{sgn}(v_1)\scdot e_1\scdot \norm{v}$}~gewählt ($\text{sgn}(.)$ ist die Vorzeichenfunktion, jedoch muss bei 0 nicht 0 genommen werden!).
	Damit wird \mbox{$ Q_u \coloneqq \1_{m\times m} -2\cdot (u\cdot u^T)/(u^T\cdot u)$} definiert, welche $A$ so verdrehspiegelt, 
	dass alle Elemente in der ersten Spalte unterhalb der Diagonalen verschwinden. \linebreak[3]	
	Nun kann man weiter vorgehen und die Teilmatrix von $A$ hernehmen, welche die erste Zeile und Spalte gestrichen hat und darauf weiter agieren. Am ende hätte man mindestens eine obere rechte Dreiecksmatrix. 
	Das Produkt aller verwendeten $Q$ wäre dann eine orthogonale Matrix, womit $Q\cdot R = A$ als QR\nobreakdash-Zerlegung entstanden ist.
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Eigenschaften der Householder-Reflexion}:
	\vspace{-\topsep}
  	\begin{itemize}
	\setlength{\itemsep}{0pt}%
	\setlength{\parskip}{0pt}
  		\item[(i)] $Q_v\cdot v = -v $
		\item[(ii)] $Q_v \cdot u=u \Leftrightarrow v \bot u$
		\item[(iii)] $Q_v^T = Q_v^{-1} \Rightarrow Q_v$ ist Orthogonal
	\end{itemize}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	Eine \textbf{Givensrotation} von $
	A = 
	\begin{pmatrix}
	a_1 & *\\
	a_2 & *
	\end{pmatrix}$
	kann mit
	$r = \abs{\sqrt{a_1^2 + a_2^2}}$, $ c = \nicefrac{a_1}{r}$, $s = \nicefrac{a_2}{r}$
	und
	$ G
	= \begin{pmatrix}
	c&s\\
	-s&c
	\end{pmatrix}
	$
	erfolgen:
	$G\cdot A = 
	\begin{pmatrix}
	r&\star\\0&\star
	\end{pmatrix}$
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Nichtlineares Ausgleichsproblem}: \linebreak[3]
	\mbox{Daten}:
	\begin{tabular}{r|c|c|c|c}
		$t_i$ & 0 & 1 & 2 \\ 
		\hline 
		$y_i$ & 2 & -3 & 4 \\ 
	\end{tabular}
	\mbox{Modellfunktion}:
	\mbox{$ y(t) = x_1 \sin(x_2 \cdot t)$}
	\begin{dmath*}
		\varphi(t, x) \hiderel{=} y(t)\\
		F_i \hiderel{=} y_i - \varphi(t_i, x)\\
		\R^{n} \ni F(x)
		\hiderel{=}
		\begin{pmatrix}
			2-0\\
			-3-x_1\sin(x_2)\\
			4-x_1\sin(2x_2)
		\end{pmatrix}\\
		J_F(x)
		\hiderel{=}
		\begin{pmatrix}
			0&0\\
			-\sin(x_2)&-x_1\cos(x_2)\\
			-\sin(2x_2)&-2x_1\cos(2x_2)
		\end{pmatrix}
	\end{dmath*}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
Algorithmus: \textbf{Gauß-Newton-Verfahren}
	\begin{algorithmic}[1]
		\State Wähle Startvektor $x^{(0)}\in\R$ 
		\For{$k = 0, 1, ... $}
			\Statex $\triangleright$ Löse LGS nach $\Delta x^{(k)}$
			\State $\norm{ J_F(x^{(k)}) \Delta x^{(k)} + F(x^{(k)}) }_2^2 \rightarrow min$
			\State setze $x^{(k+1)} = x^{(k)} + \Delta x^{(k)}$
		\EndFor
	\end{algorithmic}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	$
		A\in\R^{n\times n} \, s.p.d \Rightarrow
		\exists V \in \R^{n\times n}
	$
	orthogonal mit
	$
		V^TAV = D \,;\, d_{ii} = \lambda_i \geq 0
	$
	$\Rightarrow A = VDV^T$
	($VV^T = \1$)
	\fbox{$AV = VD$}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	$A\in \R^{m\times n}$
	$A^TA$ ist s.p.semi-d.
	$x^TA^TAx \geq 0$
	
	Satz: \textbf{Singulärwertzerlegung}: \linebreak[3]
	Sei $A\in\R^{m\times n}$ und $p = min(m, n)$. \linebreak[3]
	Dann existieren orthogonale Matrizen $U\in \R^{m\times m}$ 
	und $V\in \R^{n\times n}$ mit \linebreak[3]
	$U^TAV=\Sigma = diag(\sigma_1, ..., \sigma_p)
	\Rightarrow $\fbox{$A = U\Sigma V^T$}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	\textbf{Anwendung der SVD}
	\vspace{-\topsep}
	\begin{itemize}
	\setlength{\itemsep}{0pt}%
	\setlength{\parskip}{0pt}
		\item[(i)] $A^{(s)} \coloneqq \sum_{i=1}^{n}\sigma_iu_iv_i^T, s < r$ ist die beste (im Sinne der $\norm{.}_2$) Approximation von A mit dem Rang(s).
		\item[(ii)] \textbf{Pseudoinverse \boldmath$A^+$}, $A\in \R^{m\times n}$; \linebreak[3]
			$A = U\Sigma V^T$; \linebreak[3]
			\mbox{$A^+ = V\Sigma^+ U^T$} mit \linebreak[3]
			\mbox{$\Sigma^+ = 
				\begin{pmatrix}
					\nicefrac{1}{\sigma_1} & 0&0\\
					0 & \nicefrac{1}{\sigma_r}&0\\
					0&0&0
				\end{pmatrix}
			$}; \linebreak[3]
			\mbox{$x = A^+b$} ist die Lösung des linearen Ausgleichsproblems \mbox{$\norm{Ax-b}_2\rightarrow min$}
		\item[(iii)] Lösung des linearen Ausgleichsproblems für \underline{$Rang(A) < n$}
		\item[(iv)] Regularisierung schlecht gestellter Probleme \linebreak[3]
			\mbox{$ \hat{x} = \sum_{i=1}^{s} \nicefrac{1}{\sigma_i} v_i\scdot\left(u_i^T \scdot b\right)$}, $s < n$
	\end{itemize}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	Algorithmus: \textbf{Vektoriteration}
	\begin{algorithmic}[1]
		\label{bar label}
		\State wähle $ x^{(0)} \in \R $, setze $y^{(0)} = \nicefrac{x^{(0)}}{\norm{x^{(0)}}_2}$
		\For{$k = 0, 1, ... $}
		\State $ x^{k+1} = A\cdot y^{(k)}$
		\State $\lambda^{(k)} = {y^{(k)}}^T \cdot x^{(k)}$
		\State $ y^{k+1} = \nicefrac{x^{(k+1)}}{\norm{x^{(k+1)}}_2}$
		\EndFor
	\end{algorithmic}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	Algorithmus: \textbf{Inverse Vektoriteration mit Spektralverschiebung}
	\begin{algorithmic}[1]
		\label{bar label}
		\State wähle $ x^{(0)} \in \R $, setze $y^{(0)} = \nicefrac{x^{(0)}}{\norm{x^{(0)}}_2}$
		\For{$k = 0, 1, ... $}
		\State Löse LGS \;$ \left( A-\mu\1 \right) x^{(k+1)} = y^{(k)} $ 
		\Statex{$\triangleright\quad\Leftrightarrow x^{(k+1)} = \left( A-\mu\1 \right)^{-1} \cdot y^{(k)}$}
		\State $\lambda^{(k+1)} = \nicefrac{1}{{y^{(k)}}^T \cdot x^{(k)}} + \mu$
		\State $ y^{k+1} = \nicefrac{x^{(k+1)}}{\norm{x^{(k+1)}}_2}$
		\EndFor
	\end{algorithmic}
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
	Lemma zum QR-Verfahren
	\vspace{-\topsep}
	\begin{itemize}
		\setlength{\itemsep}{0pt}%
		\setlength{\parskip}{0pt}
		\item[(i)] die Matrizen $A_k$ sind \textbf{ähnlich} zu A.
		\item[(ii)] $A$ Symetrisch $\Rightarrow$ $A_k$ Symmetrisch.
		\item[(iii)] $A$ tridiagonal und Symetrisch $\Rightarrow$ $A_k$ auch.
	\end{itemize}
	\vspace{-\topsep}
	Zwei Matrizen $A$ und $B$ sind sich \textbf{ähnlich}, wenn es eine reguläre Matrix $S$ gibt, so dass: 
	\mbox{$B = S^{-1} \scdot A \scdot S$} \mbox{$\Leftrightarrow SB=AS$} gilt. \linebreak[3]
	Ähnliche Matrizen haben dasselbe Spektrum $\sigma(A) = \sigma(B)$, gleiche Spur, gleichen Rang aber nicht notwendigerweise die gleichen EV.
\end{minipage}}
\fbox{\begin{minipage}[tc]{\linewidth-2\fboxsep}\ifthenelse{\boolean{doraggedright}}{\raggedright}{}
Algorithmus: \textbf{QR-Verfahren mit Spektralverschiebung}
	\begin{algorithmic}[1]
		\State $A_0 = P^T\cdot A\cdot P$ 
		\Statex{$\triangleright\quad$ Tridiagonaltransformation}
		\For{$k = 0, 1, ... $}
		\State wähle $\mu_k\in\R$
		\State $A_k -\mu_k\1 = Q_k\cdot R_k$
		\Statex{$\triangleright\quad$ QR-Zerlegung}
		\State $A_{k+1} = R_k \cdot Q_k + \mu_k\1 $ 
		\Statex{$\triangleright\quad = Q_k^T A_k Q_k$}
		\EndFor
	\end{algorithmic}
\end{minipage}}







































